{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/optimopium/is-this-political/blob/main/Discrepancy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSERabNzxhVW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kptJfBRw3QpP"
      },
      "source": [
        "### Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wFee-4I24i_"
      },
      "outputs": [],
      "source": [
        "! pip install --quiet -U transformers\n",
        "! pip install --quiet datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgDvFqUD3SKv"
      },
      "source": [
        "### Prelimenaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJKJOr4qxqDZ"
      },
      "outputs": [],
      "source": [
        "base_dir = 'drive/MyDrive/Thesis/Data/annotated/'\n",
        "data_files = {\"train\": base_dir + \"train.csv\", \"validation\": base_dir + \"validation.csv\", \"test\": base_dir + \"test.csv\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16mtNjDA5PZb"
      },
      "outputs": [],
      "source": [
        "! mkdir results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGgKnSjO4Cii"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGVVQHUF_-LM"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsnHRxTO6TMx"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSfZAJVT3ZME"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "# Creating an object\n",
        "logger = logging.getLogger()\n",
        " \n",
        "# Setting the threshold of logger to DEBUG\n",
        "logger.setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lnHxEBl3o-h"
      },
      "outputs": [],
      "source": [
        "# General variables\n",
        "MAX_SEQ_LEN = 128\n",
        "UNLABELED_BATCH_SIZE = 32\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 16\n",
        "TEST_BATCH_SIZE = 16\n",
        "LEARNING_RATE = 5e-5\n",
        "EPS = 1e-8\n",
        "LOGGING_STEPS = 50\n",
        "EVAL_STEPS = 30\n",
        "D_IN = 768\n",
        "D_OUT = 2\n",
        "# epoch * (budget/batch size)\n",
        "BASE_MODEL = 'xlm-roberta-base'\n",
        "MAX_STEPS = 500\n",
        "EPOCHS = int(MAX_STEPS/TRAIN_BATCH_SIZE)\n",
        "MODEL_OUTPUT_DIR = \"results/model/\"\n",
        "CLASSIFIER_ONE_OUTPUT_DIR = \"results/classifier1/\"\n",
        "CLASSIFIER_TWO_OUTPUT_DIR = \"results/classifier2/\"\n",
        "CLASSIFIER_DROP_OUT = 0.25\n",
        "CLASSIFIER_HIDDEN_LAYER_SIZE = 100\n",
        "\n",
        "BUDGET = 1125\n",
        "INITIAL_DATASET_SIZE = 125\n",
        "ACQUISITION_SIZE = 100\n",
        "ITERATIONS = int((BUDGET - INITIAL_DATASET_SIZE) / ACQUISITION_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0RW3mqe_qw2"
      },
      "outputs": [],
      "source": [
        "def freeze_model(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "def unfreeze_model(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYcheTIV3-S2"
      },
      "source": [
        "### Discrepancy Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4jHDqE_W7Aq"
      },
      "outputs": [],
      "source": [
        "l1 = nn.L1Loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugAb5-Q-4FL7"
      },
      "outputs": [],
      "source": [
        "def discrepancy_loss(logits1, logits2, logits3):\n",
        "    def discrepancy(outputs1, outputs2):\n",
        "        return l1(outputs1, outputs2)\n",
        "    loss = (discrepancy(logits1, logits2) + discrepancy(logits1, logits3) + discrepancy(logits2, logits3)) / 3\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1LovOVB30uW"
      },
      "outputs": [],
      "source": [
        "a = torch.Tensor([-0.6909,  1.1088])\n",
        "b = torch.Tensor([-0.6909,  1.1188])\n",
        "c = torch.Tensor([-0.6909,  1.1188])\n",
        "discrepancy_loss(a, b, c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEdvbnWx5jIz"
      },
      "source": [
        "### Cross Entorpy Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_Ox6QWI5mAQ"
      },
      "outputs": [],
      "source": [
        "ce_loss = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26B_XEfZ7kpY"
      },
      "source": [
        "### Create Dataset Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zBPMEl0yqgw"
      },
      "outputs": [],
      "source": [
        "from datasets import ClassLabel\n",
        "\n",
        "c2l = ClassLabel(num_classes=2, names=['Nonpolitical', 'Political'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhCbSaiu7ol1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "def transform_to_embedding_dataset(tokenizer, dataset, is_bert=False):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    token_type_ids = []\n",
        "    labels = []\n",
        "\n",
        "    for item in dataset:\n",
        "        # `encode_plus` will:\n",
        "        #   (1) Tokenize the sentence.\n",
        "        #   (2) Prepend the `[CLS]` token to the start.\n",
        "        #   (3) Append the `[SEP]` token to the end.\n",
        "        #   (4) Map tokens to their IDs.\n",
        "        #   (5) Pad or truncate the sentence to `max_length`\n",
        "        #   (6) Create attention masks for [PAD] tokens.\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                            item[\"sentence1\"],                      # Sentence to encode.\n",
        "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                            max_length = 256,           # Pad & truncate all sentences.\n",
        "                            pad_to_max_length = True,\n",
        "                            return_attention_mask = True,   # Construct attn. masks.\n",
        "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "        \n",
        "        input_ids.append(encoded_dict[\"input_ids\"])\n",
        "        attention_masks.append(encoded_dict[\"attention_mask\"])\n",
        "        if is_bert:\n",
        "            token_type_ids.append(encoded_dict[\"token_type_ids\"])\n",
        "\n",
        "        labels.append(c2l.str2int(item[\"label\"]))\n",
        "\n",
        "\n",
        "    # Convert the lists into tensors.\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    if is_bert:\n",
        "        token_type_ids = torch.cat(token_type_ids, dim=0)\n",
        "    labels = torch.tensor(labels)\n",
        "    \n",
        "\n",
        "\n",
        "    # Combine the training inputs into a TensorDataset.\n",
        "    if is_bert:\n",
        "        dataset = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
        "    else:\n",
        "        dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SPHQnXA4cwW"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFMagoUg4KQA"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoModel\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjnZqS9Y9XT0"
      },
      "source": [
        "### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjNLUpCzvu7D"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from torch.utils.data import SequentialSampler, DataLoader\n",
        "\n",
        "def train(train_dataset, validation_dataset, is_bert=False):\n",
        "    # Creating dataloaders\n",
        "    logging.info(\"Creating Dataloaders.\")\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    logging.info(\"Instantiating models.\")\n",
        "    base_model = AutoModel.from_pretrained(BASE_MODEL).to(DEVICE)\n",
        "    head = nn.Sequential(\n",
        "            nn.Dropout(CLASSIFIER_DROP_OUT),\n",
        "            nn.Linear(D_IN, CLASSIFIER_HIDDEN_LAYER_SIZE),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(CLASSIFIER_DROP_OUT),\n",
        "            nn.Linear(CLASSIFIER_HIDDEN_LAYER_SIZE, D_OUT),\n",
        "        ).to(DEVICE)\n",
        "\n",
        "    logging.info(\"Optimizer and Scheduler setup.\")\n",
        "    optimizer = AdamW([\n",
        "                    {'params': base_model.parameters()},\n",
        "                    {'params': head.parameters()}\n",
        "                ], lr=LEARNING_RATE, eps=EPS)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=MAX_STEPS)\n",
        "\n",
        "    # Start training loop\n",
        "    logging.info(\"Training\")\n",
        "    for epoch_i in tqdm(range(EPOCHS)):\n",
        "        # Print the header of the result table\n",
        "        print(f\"\\n{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12}\")\n",
        "        print(\"-\"*45)\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        base_model.train()\n",
        "        head.train()\n",
        "\n",
        "        for step, t_batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "\n",
        "            t_batch = tuple(t.to(DEVICE) for t in t_batch)\n",
        "            if is_bert:\n",
        "                inputs = {\"input_ids\": t_batch[0], \"attention_mask\": t_batch[1], \"token_type_ids\": t_batch[2]}\n",
        "            else:\n",
        "                inputs = {\"input_ids\": t_batch[0], \"attention_mask\": t_batch[1]}\n",
        "\n",
        "\n",
        "            # Update Primary Model\n",
        "            unfreeze_model(base_model)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            base_outputs = base_model(**inputs)\n",
        "            last_hidden_state_cls = base_outputs[0][:, 0, :]\n",
        "            head_outputs = head(last_hidden_state_cls)\n",
        "\n",
        "            if is_bert:\n",
        "                head_loss = ce_loss(head_outputs, t_batch[3])\n",
        "            else:\n",
        "                head_loss = ce_loss(head_outputs, t_batch[2])\n",
        "\n",
        "            head_loss.backward()\n",
        "\n",
        "            batch_loss += head_loss\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(base_model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        # Print the loss values and time elapsed for every 20 batches\n",
        "        if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "            # Print training results\n",
        "            print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f}\")\n",
        "\n",
        "            # Reset batch tracking variables\n",
        "            batch_loss, batch_counts = 0, 0\n",
        "\n",
        "        # After the completion of each training epoch, measure the model's performance\n",
        "        # on our validation set.\n",
        "        head_val_loss, head_val_accuracy = evaluate(base_model, head, validation_dataset)\n",
        "        logging.info(f\"Evaluation Head:: Loss={head_val_loss} Accuracy={head_val_accuracy}\")\n",
        "\n",
        "        return base_model, head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80XfxLhY9Y0s"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def discrepancy_train(train_dataset, validation_dataset, unlabeled_dataset, is_bert=False):\n",
        "    # Creating dataloaders\n",
        "    logging.info(\"Creating Dataloaders.\")\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
        "    unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size=UNLABELED_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    logging.info(\"Instantiating models.\")\n",
        "    base_model = AutoModel.from_pretrained(BASE_MODEL).to(DEVICE)\n",
        "    head = nn.Sequential(\n",
        "            nn.Dropout(CLASSIFIER_DROP_OUT),\n",
        "            nn.Linear(D_IN, CLASSIFIER_HIDDEN_LAYER_SIZE),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(CLASSIFIER_DROP_OUT),\n",
        "            nn.Linear(CLASSIFIER_HIDDEN_LAYER_SIZE, D_OUT),\n",
        "        ).to(DEVICE)\n",
        "    classifier1 = nn.Sequential(\n",
        "            nn.Dropout(CLASSIFIER_DROP_OUT),\n",
        "            nn.Linear(D_IN, CLASSIFIER_HIDDEN_LAYER_SIZE),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(CLASSIFIER_DROP_OUT),\n",
        "            nn.Linear(CLASSIFIER_HIDDEN_LAYER_SIZE, D_OUT),\n",
        "        ).to(DEVICE)\n",
        "    classifier2 = nn.Sequential(\n",
        "            nn.Dropout(CLASSIFIER_DROP_OUT),\n",
        "            nn.Linear(D_IN, CLASSIFIER_HIDDEN_LAYER_SIZE),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(CLASSIFIER_DROP_OUT),\n",
        "            nn.Linear(CLASSIFIER_HIDDEN_LAYER_SIZE, D_OUT),\n",
        "        ).to(DEVICE)\n",
        "\n",
        "\n",
        "    logging.info(\"Optimizer and Scheduler setup.\")\n",
        "    optimizer = AdamW([\n",
        "                    {'params': base_model.parameters()},\n",
        "                    {'params': head.parameters()}\n",
        "                ], lr=LEARNING_RATE, eps=EPS)\n",
        "    clf_optimizer = AdamW([\n",
        "                    {'params': classifier1.parameters()},\n",
        "                    {'params': classifier2.parameters()}\n",
        "                ], lr=LEARNING_RATE, eps=EPS)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=MAX_STEPS)\n",
        "\n",
        "    logging.info(\"Discrepancy Training\")\n",
        "\n",
        "    for epoch_i in tqdm(range(EPOCHS)):\n",
        "        # Print the header of the result table\n",
        "        print(f\"\\n{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12}\")\n",
        "        print(\"-\"*45)\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        base_model.train()\n",
        "        head.train()\n",
        "        classifier1.train()\n",
        "        classifier2.train()\n",
        "\n",
        "        for step, (t_batch, u_batch) in enumerate(zip(train_dataloader, unlabeled_dataloader)):\n",
        "            batch_counts +=1\n",
        "\n",
        "            t_batch = tuple(t.to(DEVICE) for t in t_batch)\n",
        "            if is_bert:\n",
        "                inputs = {\"input_ids\": t_batch[0], \"attention_mask\": t_batch[1], \"token_type_ids\": t_batch[2]}\n",
        "            else:\n",
        "                inputs = {\"input_ids\": t_batch[0], \"attention_mask\": t_batch[1]}\n",
        "\n",
        "            # Update Primary Model\n",
        "            unfreeze_model(base_model)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            base_outputs = base_model(**inputs)\n",
        "            last_hidden_state_cls = base_outputs[0][:, 0, :]\n",
        "            last_hidden_state_cls_detached = last_hidden_state_cls.detach()\n",
        "            head_outputs = head(last_hidden_state_cls)\n",
        "\n",
        "            if is_bert:\n",
        "                head_loss = ce_loss(head_outputs, t_batch[3])\n",
        "            else:\n",
        "                head_loss = ce_loss(head_outputs, t_batch[2])\n",
        "\n",
        "            head_loss.backward()\n",
        "\n",
        "            batch_loss += head_loss\n",
        "\n",
        "            # torch.nn.utils.clip_grad_norm_(base_model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Update Auxlary Models\n",
        "            freeze_model(base_model)\n",
        "            clf_optimizer.zero_grad()\n",
        "\n",
        "            classifier1_outputs = classifier1(last_hidden_state_cls_detached)\n",
        "            classifier2_outputs = classifier2(last_hidden_state_cls_detached)\n",
        "\n",
        "            if is_bert:\n",
        "                classifier1_loss = ce_loss(classifier1_outputs, t_batch[3])\n",
        "                classifier2_loss = ce_loss(classifier2_outputs, t_batch[3])\n",
        "            else:\n",
        "                classifier1_loss = ce_loss(classifier1_outputs, t_batch[2])\n",
        "                classifier2_loss = ce_loss(classifier2_outputs, t_batch[2])\n",
        "\n",
        "            classifiers_loss = classifier1_loss + classifier2_loss\n",
        "\n",
        "            classifiers_loss.backward()\n",
        "            clf_optimizer.step()\n",
        "\n",
        "\n",
        "            # Unlabled Data\n",
        "            u_batch = tuple(t.to(DEVICE) for t in u_batch)\n",
        "            if is_bert:\n",
        "                u_inputs = {\"input_ids\": u_batch[0], \"attention_mask\": u_batch[1], \"token_type_ids\": u_batch[2]}\n",
        "            else:\n",
        "                u_inputs = {\"input_ids\": u_batch[0], \"attention_mask\": u_batch[1]}\n",
        "\n",
        "            base_outputs = base_model(**u_inputs)\n",
        "            last_hidden_state_cls = base_outputs[0][:, 0, :].detach()\n",
        "\n",
        "            head_outputs = head(last_hidden_state_cls)\n",
        "            classifier1_outputs = classifier1(last_hidden_state_cls)\n",
        "            classifier2_outputs = classifier2(last_hidden_state_cls)\n",
        "\n",
        "\n",
        "            base_outputs = base_model(**inputs)\n",
        "            last_hidden_state_cls = base_outputs[0][:, 0, :]\n",
        "            last_hidden_state_cls_detached = last_hidden_state_cls.detach()\n",
        "            classifier1_outputs = classifier1(last_hidden_state_cls_detached)\n",
        "            classifier2_outputs = classifier2(last_hidden_state_cls_detached)\n",
        "\n",
        "            if is_bert:\n",
        "                classifier1_loss = ce_loss(classifier1_outputs, t_batch[3])\n",
        "                classifier2_loss = ce_loss(classifier2_outputs, t_batch[3])\n",
        "            else:\n",
        "                classifier1_loss = ce_loss(classifier1_outputs, t_batch[2])\n",
        "                classifier2_loss = ce_loss(classifier2_outputs, t_batch[2])\n",
        "            classifiers_loss = classifier1_loss + classifier2_loss\n",
        "\n",
        "            loss_dis = classifiers_loss - discrepancy_loss(head_outputs, classifier1_outputs, classifier2_outputs)\n",
        "\n",
        "            clf_optimizer.zero_grad()\n",
        "            loss_dis.backward()\n",
        "            clf_optimizer.step()\n",
        "\n",
        "\n",
        "        # Print the loss values and time elapsed for every 20 batches\n",
        "        if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "            # Print training results\n",
        "            print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f}\")\n",
        "\n",
        "            # Reset batch tracking variables\n",
        "            batch_loss, batch_counts = 0, 0\n",
        "\n",
        "        # After the completion of each training epoch, measure the model's performance\n",
        "        # on our validation set.\n",
        "        head_val_loss, head_val_accuracy = evaluate(base_model, head, validation_dataset)\n",
        "        logging.info(f\"Evaluation Head:: Loss={head_val_loss} Accuracy={head_val_accuracy}\")\n",
        "        classifier1_val_loss, classifier1_val_accuracy = evaluate(base_model, classifier1, validation_dataset)\n",
        "        logging.info(f\"Evaluation Classifier 1:: Loss={classifier1_val_loss} Accuracy={classifier1_val_accuracy}\")\n",
        "        classifier2_val_loss, classifier2_val_accuracy = evaluate(base_model, classifier2, validation_dataset)\n",
        "        logging.info(f\"Evaluation Classifier 2:: Loss={classifier2_val_loss} Accuracy={classifier2_val_accuracy}\")\n",
        "\n",
        "        return base_model, head, classifier1, classifier2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNgUCsfZ-mUy"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMtvXmuqAdV4"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, classifier, validation_dataset, is_bert=False):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size=EVAL_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "    classifier.eval()\n",
        "\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    for _ , batch in enumerate(validation_dataloader):        \n",
        "        batch = tuple(t.to(DEVICE) for t in batch)\n",
        "        if is_bert:\n",
        "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"token_type_ids\": batch[2]}\n",
        "        else:\n",
        "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1]}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "            logits = classifier(last_hidden_state_cls)\n",
        "\n",
        "        # Compute loss\n",
        "        if is_bert:\n",
        "            loss = ce_loss(logits, batch[3])\n",
        "        else:\n",
        "            loss = ce_loss(logits, batch[2])\n",
        "        val_loss.append(loss.item())\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "        # Calculate the accuracy rate\n",
        "        if is_bert:\n",
        "            accuracy = (preds == batch[3]).cpu().numpy().mean() * 100\n",
        "        else:\n",
        "            accuracy = (preds == batch[2]).cpu().numpy().mean() * 100\n",
        "\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXWiM7kaDL0N"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgaG3ch2NGEp"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def performance_check(model, head, test_dataset, classifier1=None, classifier2=None, is_bert=False, is_discrepancy=False):\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=True)\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    head.eval()\n",
        "    if is_discrepancy:\n",
        "        classifier1.eval()\n",
        "        classifier2.eval()\n",
        "\n",
        "    all_head_logits, all_logits1, all_logits2, labels = [], [], [], []\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "        batch = tuple(t.to(DEVICE) for t in batch)\n",
        "        if is_bert:\n",
        "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"token_type_ids\": batch[2]}\n",
        "        else:\n",
        "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1]}\n",
        "\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "            head_logits = head(last_hidden_state_cls)\n",
        "            if is_discrepancy:\n",
        "                logits1 = classifier1(last_hidden_state_cls)\n",
        "                logits2 = classifier2(last_hidden_state_cls)\n",
        "\n",
        "        all_head_logits.append(head_logits)\n",
        "\n",
        "        if is_discrepancy:\n",
        "            all_logits1.append(logits1)\n",
        "            all_logits2.append(logits2)\n",
        "\n",
        "        if is_bert:\n",
        "            labels.append(batch[3])\n",
        "        else:\n",
        "            labels.append(batch[2])\n",
        "\n",
        "\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_head_logits = torch.cat(all_head_logits, dim=0)\n",
        "\n",
        "    if is_discrepancy:\n",
        "        all_logits1 = torch.cat(all_logits1, dim=0)\n",
        "        all_logits2 = torch.cat(all_logits2, dim=0)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "\n",
        "    preds_head = torch.argmax(all_head_logits, dim=1).flatten()\n",
        "    if is_discrepancy:\n",
        "        preds1 = torch.argmax(all_logits1, dim=1).flatten()\n",
        "        preds2 = torch.argmax(all_logits2, dim=1).flatten()\n",
        "        return preds_head, preds1, preds2, labels\n",
        "    else:\n",
        "        return preds_head, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_k5M0BTGnh9"
      },
      "source": [
        "### Metric Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaI8kmh2G3kN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
        "\n",
        "def report(preds, labels):\n",
        "    print(\"Metrics Report:\\n\")\n",
        "    print(f\"Accuracy: {accuracy_score(labels, preds)}\")\n",
        "    print(f\"Confusion Matrix:\\n{confusion_matrix(labels, preds)}\")\n",
        "    print(f\"Precision: {precision_score(labels, preds)}\")\n",
        "    print(f\"Recall: {recall_score(labels, preds)}\")\n",
        "    print(f\"F1-score: {f1_score(labels, preds)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5UqaNMdNJ6x"
      },
      "source": [
        "### Active Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO2AVEf3t2Gt"
      },
      "outputs": [],
      "source": [
        "kl_loss = nn.KLDivLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pbhLcGLtcZm"
      },
      "outputs": [],
      "source": [
        "def disagreement_score(logits1, logits2, logits3):\n",
        "    def discrepancy(outputs1, outputs2):\n",
        "        return kl_loss(outputs1, outputs2)\n",
        "    return (discrepancy(logits1, logits2) + discrepancy(logits1, logits3) + discrepancy(logits2, logits3)) / 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnqBxN6Yq9ct"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def calculate_disagreement_score(model, test_dataset, head, classifier1=None, classifier2=None, is_bert=False, is_discrepancy=False):\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    head.eval()\n",
        "    classifier1.eval()\n",
        "    classifier2.eval()\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "        batch = tuple(t.to(DEVICE) for t in batch)\n",
        "        if is_bert:\n",
        "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"token_type_ids\": batch[2]}\n",
        "        else:\n",
        "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1]}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "            head_logits = head(last_hidden_state_cls)\n",
        "            logits1 = classifier1(last_hidden_state_cls)\n",
        "            logits2 = classifier2(last_hidden_state_cls)\n",
        "\n",
        "            scores.append(disagreement_score(head_logits, logits1, logits2))\n",
        "\n",
        "    return torch.Tensor(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9T88UGM24K6"
      },
      "outputs": [],
      "source": [
        "def annotate(unlabled_samples):\n",
        "    return unlabled_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8HeqHR8XZe3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import concatenate_datasets\n",
        "import gc\n",
        "\n",
        "def discrepancy_active_learning(\n",
        "        hf_args,\n",
        "        raw_datasets,\n",
        "        initial_labeled_dataset_size,\n",
        "        iteration_count,\n",
        "        iteration_sample_count,\n",
        "        candidate_to_selected_samples_ratio=10\n",
        "    ):\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, do_lower_case=True)\n",
        "\n",
        "    original_train_dataset = raw_datasets[\"train\"]\n",
        "\n",
        "    # select initial train dataset from raw dataset\n",
        "    train_dataset = original_train_dataset.select(\n",
        "        random.sample(\n",
        "            range(original_train_dataset.num_rows),\n",
        "            initial_labeled_dataset_size,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    unlabeled_dataset = original_train_dataset.filter(\n",
        "        lambda s: s[\"idx\"] not in train_dataset[\"idx\"]\n",
        "    )\n",
        "\n",
        "    raw_datasets[\"train\"] = train_dataset\n",
        "    # Train Initial Model\n",
        "    logger.info(f'Initial Training with {raw_datasets[\"train\"].num_rows} samples.')\n",
        "    train_labeled = transform_to_embedding_dataset(tokenizer, raw_datasets[\"train\"], is_bert=False)\n",
        "    validation_labeled = transform_to_embedding_dataset(tokenizer, raw_datasets[\"validation\"], is_bert=False)\n",
        "    model, head = train(train_dataset=train_labeled, validation_dataset=validation_labeled)\n",
        "\n",
        "    test_labeled = transform_to_embedding_dataset(tokenizer, raw_datasets[\"test\"], is_bert=False)\n",
        "    head_preds, labels = performance_check(\n",
        "        model=model,\n",
        "        head=head,\n",
        "        test_dataset=test_labeled\n",
        "    )\n",
        "    report(head_preds.cpu().numpy(), labels.cpu().numpy())\n",
        "    del model\n",
        "    del head\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    current_iteration = 1\n",
        "    while  current_iteration <= iteration_count:\n",
        "        print(f'Current Active Learning Iteration: {current_iteration}')\n",
        "\n",
        "        if unlabeled_dataset.num_rows <= 0:\n",
        "            logger.info(f'Not enough unlabeled data to continue. Stoped at iteration {current_iteration}')\n",
        "\n",
        "        # Sample candidate_to_selected_samples_ratio larger than iteration_sample_count sample for acquisition function\n",
        "        candidate_count = int(len(raw_datasets[\"train\"]))\n",
        "        logger.info(f\"Candidate samples count for active learning : {candidate_count}\")\n",
        "        candidate_samples = unlabeled_dataset.select(\n",
        "            random.sample(\n",
        "                range(unlabeled_dataset.num_rows),\n",
        "                candidate_count,\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "        # Acquisition Function - Contranstive Strategy\n",
        "        model = AutoModel.from_pretrained(BASE_MODEL).to(DEVICE)\n",
        "        train_labeled = transform_to_embedding_dataset(tokenizer, raw_datasets[\"train\"], is_bert=False)\n",
        "        validation_labeled = transform_to_embedding_dataset(tokenizer, raw_datasets[\"validation\"], is_bert=False)\n",
        "        tpool = transform_to_embedding_dataset(tokenizer, candidate_samples, is_bert=False)\n",
        "        logging.info(\"Discrepancy Training.\")\n",
        "        base_model, head, classifier1, classifier2 = discrepancy_train(train_dataset=train_labeled,\n",
        "                                                                       validation_dataset=validation_labeled,\n",
        "                                                                       unlabeled_dataset=tpool)\n",
        "        \n",
        "        tunlabeled_dataset = transform_to_embedding_dataset(tokenizer, unlabeled_dataset, is_bert=False)\n",
        "        logging.info(\"Calculating Discrepancy Scores.\")\n",
        "        sample_discrepancy_scores = calculate_disagreement_score(\n",
        "            model=base_model,\n",
        "            head=head,\n",
        "            test_dataset=tunlabeled_dataset,\n",
        "            classifier1=classifier1,\n",
        "            classifier2=classifier2,\n",
        "            is_discrepancy=True\n",
        "        )\n",
        "        chosen_samples = torch.topk(sample_discrepancy_scores, iteration_sample_count)\n",
        "        del base_model\n",
        "        del head\n",
        "        del classifier1\n",
        "        del classifier2\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Annotate new samples\n",
        "        new_train_samples = unlabeled_dataset.select(chosen_samples.indices.tolist())\n",
        "        new_train_samples = annotate(new_train_samples)\n",
        "\n",
        "\n",
        "        # Add new samples to labeled dataset\n",
        "        extended_train_dataset = concatenate_datasets(\n",
        "            [raw_datasets[\"train\"], new_train_samples],\n",
        "            info=original_train_dataset.info,\n",
        "        )\n",
        "        \n",
        "        # Remove selected samples from unlabeled dataset\n",
        "        unlabeled_dataset = original_train_dataset.filter(\n",
        "            lambda s: s[\"idx\"] not in new_train_samples[\"idx\"]\n",
        "        )\n",
        "\n",
        "        # Train new model with new dataset\n",
        "        raw_datasets[\"train\"] = extended_train_dataset\n",
        "        train_labeled = transform_to_embedding_dataset(tokenizer, raw_datasets[\"train\"], is_bert=False)\n",
        "        validation_labeled = transform_to_embedding_dataset(tokenizer, raw_datasets[\"validation\"], is_bert=False)\n",
        "        logging.info(f\"Results After Iteration {current_iteration}.\")\n",
        "        model, head = train(train_dataset=train_labeled, validation_dataset=validation_labeled)\n",
        "        test_labeled = transform_to_embedding_dataset(tokenizer, raw_datasets[\"test\"], is_bert=False)\n",
        "        head_preds, labels = performance_check(\n",
        "            model=model,\n",
        "            head=head,\n",
        "            test_dataset=test_labeled\n",
        "        )\n",
        "        if current_iteration < iteration_count:\n",
        "            del model\n",
        "            del head\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "        report(head_preds.cpu().numpy(), labels.cpu().numpy())\n",
        "                \n",
        "        current_iteration += 1\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return model, head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9isY6l_arG9"
      },
      "outputs": [],
      "source": [
        "def run_discrepancy_sampling():\n",
        "    hf_args = {\n",
        "        \"model_name_or_path\": BASE_MODEL,\n",
        "        \"do_train\": True,\n",
        "        \"do_eval\": True,\n",
        "        \"do_predict\": True,\n",
        "        \"max_seq_length\": MAX_SEQ_LEN,\n",
        "        \"per_device_train_batch_size\": TRAIN_BATCH_SIZE,\n",
        "        \"per_device_eval_batch_size\": EVAL_BATCH_SIZE,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"overwrite_output_dir\": True,\n",
        "        \"output_dir\": f\"./results/discrepancy/\",\n",
        "        \"logging_strategy\": \"steps\",\n",
        "        \"logging_steps\": LOGGING_STEPS,\n",
        "        \"evaluation_strategy\": \"steps\",\n",
        "        \"eval_steps\": EVAL_STEPS,\n",
        "        \"seed\": 12,\n",
        "        \"max_steps\": MAX_STEPS,\n",
        "        \"load_best_model_at_end\": True\n",
        "    }\n",
        "    \n",
        "    raw_datasets = load_dataset(\"csv\", data_files=data_files)\n",
        "    model, head = discrepancy_active_learning(\n",
        "        hf_args,\n",
        "        raw_datasets,\n",
        "        initial_labeled_dataset_size=INITIAL_DATASET_SIZE,\n",
        "        iteration_count=ITERATIONS,\n",
        "        iteration_sample_count=ACQUISITION_SIZE,\n",
        "        candidate_to_selected_samples_ratio=5\n",
        "    )\n",
        "\n",
        "    return model, head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4nbODMka4NF"
      },
      "outputs": [],
      "source": [
        "model, head = run_discrepancy_sampling()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkf1Jgt4OAqQ"
      },
      "source": [
        "### Test Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, do_lower_case=True)"
      ],
      "metadata": {
        "id": "wYtVX02zJhfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MLIcu9sVy35"
      },
      "outputs": [],
      "source": [
        "def is_political(sentence, base_model, classifier):\n",
        "    input = tokenizer.encode_plus(\n",
        "                sentence,                      # Sentence to encode.\n",
        "                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                max_length = 256,           # Pad & truncate all sentences.\n",
        "                pad_to_max_length = True,\n",
        "                return_attention_mask = True,   # Construct attn. masks.\n",
        "                return_tensors = 'pt',     # Return pytorch tensors.\n",
        "        ).to(DEVICE)\n",
        "    outputs = base_model(**input)\n",
        "    last_hidden_state_cls = outputs[0][:, 0, :].to(DEVICE)\n",
        "\n",
        "    logits = classifier(last_hidden_state_cls)\n",
        "\n",
        "    is_political = torch.argmax(logits, dim=1).flatten().item()\n",
        "\n",
        "    if is_political == 0:\n",
        "        return \"ðŸ‘Ž\"\n",
        "    else:\n",
        "        return \"ðŸ‘\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PSYWZH_bvdP"
      },
      "outputs": [],
      "source": [
        "is_political(\"Ø²Ù†Ø¯Ø§Ù†ÛŒØ§Ù† Ø³ÛŒØ§Ø³ÛŒ Ø±Ø§ Ø¢Ø²Ø§Ø¯ Ú©Ù†ÛŒØ¯.\", base_model=model, classifier=head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm8-msDNb0bW"
      },
      "outputs": [],
      "source": [
        "is_political(\"Ø±Ø§Ù‡Ù…Ù¾ÛŒÙ…Ø§ÛŒÛŒ Ø§Ù…Ø±ÙˆØ² Ù†Ø´Ø§Ù† Ø¯Ù‡Ù†Ø¯Ù‡ Ù‚Ø¯Ø±Øª Ù†Ø¸Ø§Ù… Ø¨ÙˆØ¯.\", base_model=model, classifier=head)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_political(\"Ø²Ù†Ø¯Ø§Ù†ÛŒØ§Ù† Ø¬Ø±Ø§Ø¦Ù… Ø³ÛŒØ§Ø³ÛŒ Ø±Ø§ Ø¨Ø§ÛŒØ¯ Ø¢Ø²Ø§Ø¯ Ú©Ø±Ø¯\", base_model=model, classifier=head)"
      ],
      "metadata": {
        "id": "OJAYqQz9WGPN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}