{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/optimopium/is-this-political/blob/main/Least_Confidence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq9AGaCp5Lf4"
      },
      "source": [
        "# Active Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFkNMv_14mmP"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kEWgaBbbYfb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Roto96z30-zO"
      },
      "source": [
        "Code is adapted from [here.](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py) and [here](https://github.com/adapter-hub/adapter-transformers/blob/cffdf3974ea19f49e1febe6e3f5b74be4e2d496a/examples/pytorch/text-classification/run_glue.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sg9ynBE-aQ9T"
      },
      "outputs": [],
      "source": [
        "! mkdir results\n",
        "! mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EXWkGRtv9F9"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade gdown\n",
        "!pip install --quiet -U transformers\n",
        "!pip install --quiet datasets\n",
        "!pip install --quiet scikit-learn\n",
        "!pip install --quiet evaluate\n",
        "!pip install --quiet sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3uxUA7Y7ziz"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "\n",
        "from scipy.stats import entropy\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "random.seed(\"42\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnfkalvx9oQF"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "url = \"https://drive.google.com/drive/folders/1_0qVo_iLOtjVcnybhBCOXMpguxCeUD1t\"\n",
        "gdown.download_folder(url, output=\"./\", quiet=True, use_cookies=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-18sNQWU7hoh"
      },
      "outputs": [],
      "source": [
        "# Creating an object\n",
        "logger = logging.getLogger()\n",
        " \n",
        "# Setting the threshold of logger to DEBUG\n",
        "logger.setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW5iKA4K0muK"
      },
      "outputs": [],
      "source": [
        "task_to_keys = {\n",
        "    \"politics\": (\"sentence\", None),\n",
        "}\n",
        "\n",
        "base_dir = './annotated/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw5Im1AKp5sa"
      },
      "outputs": [],
      "source": [
        "data_files = {\"train\": base_dir + \"train.csv\", \"validation\": base_dir + \"validation.csv\", \"test\": base_dir + \"test.csv\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nv2dR0lg6zVu"
      },
      "outputs": [],
      "source": [
        "# Active learning variables\n",
        "BUDGET = 1125\n",
        "INITIAL_DATASET_SIZE = 125\n",
        "ACQUISITION_SIZE = 100\n",
        "ITERATIONS = int((BUDGET - INITIAL_DATASET_SIZE) / ACQUISITION_SIZE)\n",
        "print(f\"Budget: {BUDGET}\")\n",
        "print(f\"Initial Dataset Size: {INITIAL_DATASET_SIZE}\")\n",
        "print(f\"Acquisition size: {ACQUISITION_SIZE}\")\n",
        "print(f\"Iterations: {ITERATIONS}\")\n",
        "\n",
        "# General variables\n",
        "MAX_SEQ_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "EVAL_BATCH_SIZE = 16\n",
        "LEARNING_RATE = 2e-5\n",
        "LOGGING_STEPS = 50\n",
        "EVAL_STEPS = 50\n",
        "# epoch * (budget/batch size)\n",
        "MAX_STEPS = 500\n",
        "CANDIDATE_TO_SAMPLE_RATIO=5\n",
        "BASE_MODEL = 'xlm-roberta-base'\n",
        "\n",
        "print(MAX_STEPS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEpeXNGn4dUr"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yWmumOCzlyB"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaQE7Rx0xHcJ"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataTrainingArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
        "    Using `HfArgumentParser` we can turn this class\n",
        "    into argparse arguments to be able to specify them on\n",
        "    the command line.\n",
        "    \"\"\"\n",
        "\n",
        "    task_name: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"The name of the task to train on: \" + \", \".join(task_to_keys.keys())},\n",
        "    )\n",
        "    dataset_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    dataset_config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
        "    )\n",
        "    max_seq_length: int = field(\n",
        "        default=128,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "                \"than this will be truncated, sequences shorter will be padded.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    overwrite_cache: bool = field(\n",
        "        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n",
        "    )\n",
        "    pad_to_max_length: bool = field(\n",
        "        default=True,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Whether to pad all samples to `max_seq_length`. \"\n",
        "                \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_eval_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    max_predict_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
        "                \"value if set.\"\n",
        "            )\n",
        "        },\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRXTsc8hzmkv"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelArguments:\n",
        "    \"\"\"\n",
        "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
        "    \"\"\"\n",
        "\n",
        "    model_name_or_path: str = field(\n",
        "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
        "    )\n",
        "    config_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
        "    )\n",
        "    tokenizer_name: Optional[str] = field(\n",
        "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
        "    )\n",
        "    cache_dir: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
        "    )\n",
        "    use_fast_tokenizer: bool = field(\n",
        "        default=True,\n",
        "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
        "    )\n",
        "    model_revision: str = field(\n",
        "        default=\"main\",\n",
        "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
        "    )\n",
        "    use_auth_token: bool = field(\n",
        "        default=False,\n",
        "        metadata={\n",
        "            \"help\": (\n",
        "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
        "                \"with private models).\"\n",
        "            )\n",
        "        },\n",
        "    )\n",
        "    ignore_mismatched_sizes: bool = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Will enable to load a pretrained model whose head dimensions are different.\"},\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0gMBPho25c8"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import concatenate_datasets, load_dataset, load_metric\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "import evaluate\n",
        "\n",
        "import transformers\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    EvalPrediction,\n",
        "    HfArgumentParser,\n",
        "    PretrainedConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    default_data_collator,\n",
        "    set_seed,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiiWzLHv1X4F"
      },
      "outputs": [],
      "source": [
        "def experiment(raw_datasets, args_dict=None):\n",
        "    # See all possible arguments in src/transformers/training_args.py\n",
        "    # or by passing the --help flag to this script.\n",
        "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
        "\n",
        "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
        "    if args_dict is not None:\n",
        "        model_args, data_args, training_args = parser.parse_dict(args_dict)\n",
        "    elif len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
        "        # If we pass only one argument to the script and it's the path to a json file,\n",
        "        # let's parse it to get our arguments.\n",
        "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
        "    else:\n",
        "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        handlers=[logging.StreamHandler(sys.stdout)],\n",
        "    )\n",
        "\n",
        "    log_level = training_args.get_process_log_level()\n",
        "    logger.setLevel(log_level)\n",
        "    datasets.utils.logging.set_verbosity(log_level)\n",
        "    transformers.utils.logging.set_verbosity(log_level)\n",
        "    transformers.utils.logging.enable_default_handler()\n",
        "    transformers.utils.logging.enable_explicit_format()\n",
        "\n",
        "    # Log on each process the small summary:\n",
        "    logger.warning(\n",
        "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
        "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
        "    )\n",
        "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
        "\n",
        "    # Detecting last checkpoint.\n",
        "    last_checkpoint = None\n",
        "    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
        "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
        "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
        "            raise ValueError(\n",
        "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
        "                \"Use --overwrite_output_dir to overcome.\"\n",
        "            )\n",
        "        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
        "            logger.info(\n",
        "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
        "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
        "            )\n",
        "\n",
        "    # Set seed before initializing model.\n",
        "    set_seed(training_args.seed)\n",
        "\n",
        "    # Labels\n",
        "    if data_args.task_name is not None:\n",
        "        is_regression = data_args.task_name == \"stsb\"\n",
        "        if not is_regression:\n",
        "            label_list = raw_datasets[\"train\"].features[\"label\"].names\n",
        "            num_labels = len(label_list)\n",
        "        else:\n",
        "            num_labels = 1\n",
        "    else:\n",
        "        # Trying to have good defaults here, don't hesitate to tweak to your needs.\n",
        "        is_regression = raw_datasets[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
        "        if is_regression:\n",
        "            num_labels = 1\n",
        "        else:\n",
        "            # A useful fast method:\n",
        "            # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.unique\n",
        "            label_list = raw_datasets[\"train\"].unique(\"label\")\n",
        "            label_list.sort()  # Let's sort it for determinism\n",
        "            num_labels = len(label_list)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    #\n",
        "    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
        "    # download model & vocab.\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
        "        num_labels=num_labels,\n",
        "        finetuning_task=data_args.task_name,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        revision=model_args.model_revision,\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        use_fast=model_args.use_fast_tokenizer,\n",
        "        revision=model_args.model_revision,\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\n",
        "    )\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_args.model_name_or_path,\n",
        "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
        "        config=config,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        revision=model_args.model_revision,\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\n",
        "        ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,\n",
        "    )\n",
        "\n",
        "    # Preprocessing the raw_datasets\n",
        "    if data_args.task_name is not None:\n",
        "        sentence1_key, sentence2_key = task_to_keys[data_args.task_name]\n",
        "    else:\n",
        "        # Again, we try to have some nice defaults but don't hesitate to tweak to your use case.\n",
        "        non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\n",
        "        if \"sentence1\" in non_label_column_names and \"sentence2\" in non_label_column_names:\n",
        "            sentence1_key, sentence2_key = \"sentence1\", \"sentence2\"\n",
        "        else:\n",
        "            if len(non_label_column_names) > 2:\n",
        "                sentence1_key, sentence2_key = non_label_column_names[:2]\n",
        "            else:\n",
        "                sentence1_key, sentence2_key = \"sentence1\", None\n",
        "\n",
        "    # Padding strategy\n",
        "    if data_args.pad_to_max_length:\n",
        "        padding = \"max_length\"\n",
        "    else:\n",
        "        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n",
        "        padding = False\n",
        "\n",
        "    # Some models have set the order of the labels to use, so let's make sure we do use it.\n",
        "    label_to_id = None\n",
        "    if (\n",
        "        model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id\n",
        "        and data_args.task_name is not None\n",
        "        and not is_regression\n",
        "    ):\n",
        "        # Some have all caps in their config, some don't.\n",
        "        label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n",
        "        if list(sorted(label_name_to_id.keys())) == list(sorted(label_list)):\n",
        "            label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}\n",
        "        else:\n",
        "            logger.warning(\n",
        "                f\"\"\"Your model seems to have been trained with labels, but they don't match the dataset:\\n\n",
        "                model labels: {list(sorted(label_name_to_id.keys()))}, dataset labels: {list(sorted(label_list))}.\\n\n",
        "                Ignoring the model labels as a result.\"\"\",\n",
        "            )\n",
        "    elif data_args.task_name is None and not is_regression:\n",
        "        label_to_id = {v: i for i, v in enumerate(label_list)}\n",
        "\n",
        "    if label_to_id is not None:\n",
        "        model.config.label2id = label_to_id\n",
        "        model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
        "    elif data_args.task_name is not None and not is_regression:\n",
        "        model.config.label2id = {l: i for i, l in enumerate(label_list)}\n",
        "        model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
        "\n",
        "    if data_args.max_seq_length > tokenizer.model_max_length:\n",
        "        logger.warning(\n",
        "            f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"\n",
        "            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
        "        )\n",
        "    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        # Tokenize the texts\n",
        "        args = (\n",
        "            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
        "        )\n",
        "        result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n",
        "\n",
        "        # Map labels to IDs (not necessary for GLUE tasks)\n",
        "        if label_to_id is not None and \"label\" in examples:\n",
        "            result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
        "        return result\n",
        "\n",
        "    with training_args.main_process_first(desc=\"dataset map pre-processing\"):\n",
        "        raw_datasets = raw_datasets.map(\n",
        "            preprocess_function,\n",
        "            batched=True,\n",
        "            load_from_cache_file=not data_args.overwrite_cache,\n",
        "            desc=\"Running tokenizer on dataset\",\n",
        "        )\n",
        "    if training_args.do_train:\n",
        "        if \"train\" not in raw_datasets:\n",
        "            raise ValueError(\"--do_train requires a train dataset\")\n",
        "        train_dataset = raw_datasets[\"train\"]\n",
        "        if data_args.max_train_samples is not None:\n",
        "            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
        "            train_dataset = train_dataset.select(range(max_train_samples))\n",
        "\n",
        "    if training_args.do_eval:\n",
        "        if \"validation\" not in raw_datasets and \"validation_matched\" not in raw_datasets:\n",
        "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
        "        eval_dataset = raw_datasets[\"validation_matched\" if data_args.task_name == \"mnli\" else \"validation\"]\n",
        "        if data_args.max_eval_samples is not None:\n",
        "            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
        "            eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
        "\n",
        "    if training_args.do_predict or data_args.task_name is not None or data_args.test_file is not None:\n",
        "        if \"test\" not in raw_datasets and \"test_matched\" not in raw_datasets:\n",
        "            raise ValueError(\"--do_predict requires a test dataset\")\n",
        "        predict_dataset = raw_datasets[\"test_matched\" if data_args.task_name == \"mnli\" else \"test\"]\n",
        "        if data_args.max_predict_samples is not None:\n",
        "            max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n",
        "            predict_dataset = predict_dataset.select(range(max_predict_samples))\n",
        "\n",
        "    # Log a few random samples from the training set:\n",
        "    if training_args.do_train:\n",
        "        for index in random.sample(range(len(train_dataset)), 3):\n",
        "            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
        "\n",
        "    # Get the metric function\n",
        "    if data_args.task_name is not None:\n",
        "        metric = evaluate.load(\"glue\", data_args.task_name)\n",
        "    else:\n",
        "        metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n",
        "    # predictions and label_ids field) and has to return a dictionary string to float.\n",
        "    def compute_metrics(p: EvalPrediction):\n",
        "        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
        "        preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
        "        if data_args.task_name is not None:\n",
        "            result = metric.compute(predictions=preds, references=p.label_ids)\n",
        "            if len(result) > 1:\n",
        "                result[\"combined_score\"] = np.mean(list(result.values())).item()\n",
        "            return result\n",
        "        elif is_regression:\n",
        "            return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
        "        else:\n",
        "            return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
        "\n",
        "    # Data collator will default to DataCollatorWithPadding when the tokenizer is passed to Trainer, so we change it if\n",
        "    # we already did the padding.\n",
        "    if data_args.pad_to_max_length:\n",
        "        data_collator = default_data_collator\n",
        "    elif training_args.fp16:\n",
        "        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
        "    else:\n",
        "        data_collator = None\n",
        "\n",
        "    # Initialize our Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset if training_args.do_train else None,\n",
        "        eval_dataset=eval_dataset if training_args.do_eval else None,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    if training_args.do_train:\n",
        "        checkpoint = None\n",
        "        if training_args.resume_from_checkpoint is not None:\n",
        "            checkpoint = training_args.resume_from_checkpoint\n",
        "        elif last_checkpoint is not None:\n",
        "            checkpoint = last_checkpoint\n",
        "        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
        "        metrics = train_result.metrics\n",
        "        max_train_samples = (\n",
        "            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
        "        )\n",
        "        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "\n",
        "        trainer.save_model(training_args.output_dir)  # Saves the tokenizer too for easy upload\n",
        "\n",
        "        trainer.log_metrics(\"train\", metrics)\n",
        "        trainer.save_metrics(\"train\", metrics)\n",
        "        trainer.save_state()\n",
        "\n",
        "    # Evaluation\n",
        "    evaluation_metrics = {}\n",
        "    if training_args.do_eval:\n",
        "        logger.info(\"*** Evaluate ***\")\n",
        "\n",
        "        # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "        tasks = [data_args.task_name]\n",
        "        eval_datasets = [eval_dataset]\n",
        "        if data_args.task_name == \"mnli\":\n",
        "            tasks.append(\"mnli-mm\")\n",
        "            eval_datasets.append(raw_datasets[\"validation_mismatched\"])\n",
        "\n",
        "        for eval_dataset, task in zip(eval_datasets, tasks):\n",
        "            metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
        "\n",
        "            max_eval_samples = (\n",
        "                data_args.max_eval_samples\n",
        "                if data_args.max_eval_samples is not None\n",
        "                else len(eval_dataset)\n",
        "            )\n",
        "            metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
        "\n",
        "            trainer.log_metrics(\"eval\", metrics)\n",
        "            trainer.save_metrics(\"eval\", metrics)\n",
        "\n",
        "            evaluation_metrics = metrics\n",
        "\n",
        "    test_predictions = None\n",
        "    if training_args.do_predict:\n",
        "        logger.info(\"*** Predict ***\")\n",
        "\n",
        "        # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "        tasks = [data_args.task_name]\n",
        "        predict_datasets = [predict_dataset]\n",
        "        if data_args.task_name == \"mnli\":\n",
        "            tasks.append(\"mnli-mm\")\n",
        "            predict_datasets.append(raw_datasets[\"test_mismatched\"])\n",
        "\n",
        "        for predict_dataset, task in zip(predict_datasets, tasks):\n",
        "            # Removing the `label` columns because it contains -1 and Trainer won't like that.\n",
        "            predict_dataset = predict_dataset.remove_columns(\"label\")\n",
        "            test_predictions = trainer.predict(predict_dataset, metric_key_prefix=\"predict\").predictions\n",
        "\n",
        "    return evaluation_metrics, test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vc_wN4s96ytG"
      },
      "outputs": [],
      "source": [
        "def annotate(unlabled_samples):\n",
        "    return unlabled_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkA10eEsTzTT"
      },
      "source": [
        "## Least Confidence Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WixIDjrk_ndr"
      },
      "outputs": [],
      "source": [
        "def calculate_least_confidence(logits):\n",
        "    \"\"\"\n",
        "    Chooses instances having a small margin between their most likely and second most likely predicted class\n",
        "    \"\"\"\n",
        "    def least_confidence_probablity(probs):\n",
        "        ind = np.argsort(probs)\n",
        "        return 1 - probs[ind[-1]]\n",
        "    logits = torch.nn.Softmax(dim=1)(torch.from_numpy(logits))\n",
        "    least_confidence_socres = np.apply_along_axis(lambda x: least_confidence_probablity(x), 1, logits)\n",
        "    return torch.from_numpy(least_confidence_socres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sYtv-K_-_2M"
      },
      "outputs": [],
      "source": [
        "def least_confidence_active_learning(\n",
        "        hf_args,\n",
        "        raw_datasets,\n",
        "        initial_labeled_dataset_size,\n",
        "        iteration_count,\n",
        "        iteration_sample_count,\n",
        "        candidate_to_selected_samples_ratio=3\n",
        "    ):\n",
        "\n",
        "    original_train_dataset = raw_datasets[\"train\"]\n",
        "    active_learning_data = raw_datasets\n",
        "\n",
        "    # select initial train dataset from raw dataset\n",
        "    train_dataset = original_train_dataset.select(\n",
        "        random.sample(\n",
        "            range(original_train_dataset.num_rows),\n",
        "            initial_labeled_dataset_size,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    unlabeled_dataset = original_train_dataset.filter(\n",
        "        lambda s: s[\"idx\"] not in train_dataset[\"idx\"]\n",
        "    )\n",
        "\n",
        "    raw_datasets[\"train\"] = train_dataset\n",
        "    # Train Initial Model\n",
        "    logger.info(f'Initial Training with {raw_datasets[\"train\"].num_rows} samples.')\n",
        "    evaluation_metrics, test_predictions = experiment(raw_datasets, args_dict=hf_args)\n",
        "\n",
        "    current_iteration = 1\n",
        "    while  current_iteration <= iteration_count:\n",
        "        print(f'Current Active Learning Iteration: {current_iteration}')\n",
        "\n",
        "        if unlabeled_dataset.num_rows <= 0:\n",
        "            logger.info(f'Not enough unlabeled data to continue. Stoped at iteration {current_iteration}')\n",
        "\n",
        "        # Sample candidate_to_selected_samples_ratio larger than iteration_sample_count sample for acquisition function\n",
        "        candidate_count = int(candidate_to_selected_samples_ratio * iteration_sample_count)\n",
        "        logger.info(f\"Candidate samples count for active learning : {candidate_count}\")\n",
        "        candidate_samples = unlabeled_dataset.select(\n",
        "            random.sample(\n",
        "                range(unlabeled_dataset.num_rows),\n",
        "                candidate_count,\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "        # Acquisition Function - Least Confidence Strategy\n",
        "        active_learning_data = datasets.DatasetDict({\"train\":candidate_samples, \"test\":candidate_samples})\n",
        "        hf_args[\"do_train\"] = False\n",
        "        hf_args[\"do_eval\"] = False\n",
        "        hf_args[\"do_predict\"] = True\n",
        "        hf_args[\"evaluation_strategy\"] = \"no\"\n",
        "        del(hf_args[\"load_best_model_at_end\"])\n",
        "        _, candidate_test_predictions = experiment(active_learning_data, args_dict=hf_args)\n",
        "        sample_margins = calculate_least_confidence(candidate_test_predictions)\n",
        "        chosen_samples = torch.topk(sample_margins, iteration_sample_count)\n",
        "\n",
        "        # Annotate new samples\n",
        "        new_train_samples = unlabeled_dataset.select(chosen_samples.indices.tolist())\n",
        "        new_train_samples = annotate(new_train_samples)\n",
        "\n",
        "\n",
        "        # Add new samples to labeled dataset\n",
        "        extended_train_dataset = concatenate_datasets(\n",
        "            [raw_datasets[\"train\"], new_train_samples],\n",
        "            info=original_train_dataset.info,\n",
        "        )\n",
        "        \n",
        "        # Remove selected samples from unlabeled dataset\n",
        "        unlabeled_dataset = original_train_dataset.filter(\n",
        "            lambda s: s[\"idx\"] not in new_train_samples[\"idx\"]\n",
        "        )\n",
        "\n",
        "        # Train new model with new dataset\n",
        "        raw_datasets[\"train\"] = extended_train_dataset\n",
        "        hf_args[\"do_train\"] = True\n",
        "        hf_args[\"do_eval\"] = True\n",
        "        hf_args[\"evaluation_strategy\"] = \"steps\"\n",
        "        hf_args[\"load_best_model_at_end\"] = True\n",
        "        _, candidate_test_predictions = experiment(raw_datasets, args_dict=hf_args)\n",
        "\n",
        "        current_iteration += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5q5SximjQ2yI"
      },
      "outputs": [],
      "source": [
        "def run_least_confidence_sampling():\n",
        "    hf_args = {\n",
        "        \"model_name_or_path\": BASE_MODEL,\n",
        "        \"do_train\": True,\n",
        "        \"do_eval\": True,\n",
        "        \"do_predict\": True,\n",
        "        \"max_seq_length\": MAX_SEQ_LEN,\n",
        "        \"per_device_train_batch_size\": TRAIN_BATCH_SIZE,\n",
        "        \"per_device_eval_batch_size\": EVAL_BATCH_SIZE,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"overwrite_output_dir\": True,\n",
        "        \"output_dir\": f\"./results/least_confidence/\",\n",
        "        \"logging_strategy\": \"steps\",\n",
        "        \"logging_steps\": LOGGING_STEPS,\n",
        "        \"evaluation_strategy\": \"steps\",\n",
        "        \"eval_steps\": EVAL_STEPS,\n",
        "        \"seed\": 12,\n",
        "        \"max_steps\": MAX_STEPS,\n",
        "        \"load_best_model_at_end\": True\n",
        "    }\n",
        "\n",
        "    raw_datasets = load_dataset(\"csv\", data_files=data_files)\n",
        "    least_confidence_active_learning(\n",
        "        hf_args,\n",
        "        raw_datasets,\n",
        "        initial_labeled_dataset_size=INITIAL_DATASET_SIZE,\n",
        "        iteration_count=ITERATIONS,\n",
        "        iteration_sample_count=ACQUISITION_SIZE,\n",
        "        candidate_to_selected_samples_ratio=CANDIDATE_TO_SAMPLE_RATIO\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-w0Ukc1fV-b"
      },
      "outputs": [],
      "source": [
        "run_least_confidence_sampling()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mv ./results/least_confidence/  ./drive/MyDrive/Thesis/Data/experiments/"
      ],
      "metadata": {
        "id": "GAoqHLEfb9Kx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}