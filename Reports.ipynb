{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/optimopium/is-this-political/blob/main/Reports.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq9AGaCp5Lf4"
      },
      "source": [
        "# Active Learning Reports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFkNMv_14mmP"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kEWgaBbbYfb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f8fa9a2-d8c0-4b37-8e0e-d89ea61e2280"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Roto96z30-zO"
      },
      "source": [
        "Code is adapted from [here.](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py) and [here](https://github.com/adapter-hub/adapter-transformers/blob/cffdf3974ea19f49e1febe6e3f5b74be4e2d496a/examples/pytorch/text-classification/run_glue.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Sg9ynBE-aQ9T"
      },
      "outputs": [],
      "source": [
        "! mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EXWkGRtv9F9",
        "outputId": "ed577fe1-2533-4852-9dd1-c65a02396cc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 KB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet --upgrade gdown\n",
        "!pip install --quiet -U transformers\n",
        "!pip install --quiet datasets\n",
        "!pip install --quiet scikit-learn\n",
        "!pip install --quiet evaluate\n",
        "!pip install --quiet sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i3uxUA7Y7ziz"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "from scipy.stats import entropy\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "random.seed(\"42\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-18sNQWU7hoh"
      },
      "outputs": [],
      "source": [
        "# Creating an object\n",
        "logger = logging.getLogger()\n",
        " \n",
        "# Setting the threshold of logger to DEBUG\n",
        "logger.setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "dW5iKA4K0muK"
      },
      "outputs": [],
      "source": [
        "task_to_keys = {\n",
        "    \"politics\": (\"sentence\", None),\n",
        "}\n",
        "\n",
        "base_dir = './dataset/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "url = \"https://drive.google.com/drive/folders/1B4s1JgxRKWJB4IrRVjE5wWNjM-sNSQT3\"\n",
        "gdown.download_folder(url, output=\"./\", quiet=True, use_cookies=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7bLp8B2xSyg",
        "outputId": "81b8252d-5ae9-4019-ea6b-b28a91eb8270"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./dataset/test.csv', './dataset/train.csv', './dataset/validation.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Vw5Im1AKp5sa"
      },
      "outputs": [],
      "source": [
        "data_files = {\"train\": base_dir + \"train.csv\", \"validation\": base_dir + \"validation.csv\", \"test\": base_dir + \"test.csv\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv2dR0lg6zVu",
        "outputId": "3ef35d0b-1a23-4abe-b57f-133d6d4852b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Budget: 750\n",
            "Initial Dataset Size: 150\n",
            "Acquisition size: 100\n",
            "Iterations: 6\n",
            "500\n"
          ]
        }
      ],
      "source": [
        "# Active learning variables\n",
        "BUDGET = 750\n",
        "INITIAL_DATASET_SIZE = 150\n",
        "ACQUISITION_SIZE = 100\n",
        "ITERATIONS = int((BUDGET - INITIAL_DATASET_SIZE) / ACQUISITION_SIZE)\n",
        "print(f\"Budget: {BUDGET}\")\n",
        "print(f\"Initial Dataset Size: {INITIAL_DATASET_SIZE}\")\n",
        "print(f\"Acquisition size: {ACQUISITION_SIZE}\")\n",
        "print(f\"Iterations: {ITERATIONS}\")\n",
        "\n",
        "# General variables\n",
        "MAX_SEQ_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "EVAL_BATCH_SIZE = 16\n",
        "LEARNING_RATE = 2e-5\n",
        "LOGGING_STEPS = 50\n",
        "EVAL_STEPS = 50\n",
        "# epoch * (budget/batch size)\n",
        "MAX_STEPS = 500\n",
        "CANDIDATE_TO_SAMPLE_RATIO=5\n",
        "BASE_MODEL = 'xlm-roberta-base'\n",
        "\n",
        "print(MAX_STEPS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEpeXNGn4dUr"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "j0gMBPho25c8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import SequentialSampler, DataLoader\n",
        "\n",
        "import datasets\n",
        "from datasets import concatenate_datasets, load_dataset, load_metric\n",
        "from datasets import load_dataset\n",
        "\n",
        "import transformers\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "gym2rK9uxIFA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report"
      ],
      "metadata": {
        "id": "vXPj0a4CVQOv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0ubhlGayHLj"
      },
      "source": [
        "### Load Model from Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kz0KYUoOwA5R"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "def PlotRocAuc(y_test, y_pred, color, model_name):\n",
        "    fig = go.Figure()\n",
        "      \n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=[0,1],\n",
        "            y=[0,1],\n",
        "            name=\"TPR = FPR\",\n",
        "            line=dict(color=\"black\", dash=\"dash\")\n",
        "        )\n",
        "    )\n",
        "        \n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "    auc_score = roc_auc_score(y_test,y_pred)\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=fpr,\n",
        "            y=tpr,\n",
        "            name=f\"{model_name}(AUC={auc_score})\",\n",
        "            marker=dict(color=color)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig.update_layout(title=\"ROC curve\",\n",
        "                xaxis_title=\"False Positive Rate\",\n",
        "                yaxis_title=\"True Positive Rate\")\n",
        "\n",
        "    return auc_score, fig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import ClassLabel\n",
        "\n",
        "c2l = ClassLabel(num_classes=2, names=['Nonpolitical', 'Political'])"
      ],
      "metadata": {
        "id": "zrLy2RYxw_Cd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "def transform_to_embedding_dataset(tokenizer, dataset, is_bert=True):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    token_type_ids = []\n",
        "    labels = []\n",
        "\n",
        "    for item in dataset:\n",
        "        # `encode_plus` will:\n",
        "        #   (1) Tokenize the sentence.\n",
        "        #   (2) Prepend the `[CLS]` token to the start.\n",
        "        #   (3) Append the `[SEP]` token to the end.\n",
        "        #   (4) Map tokens to their IDs.\n",
        "        #   (5) Pad or truncate the sentence to `max_length`\n",
        "        #   (6) Create attention masks for [PAD] tokens.\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                            item[\"sentence1\"],                      # Sentence to encode.\n",
        "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                            max_length = 256,           # Pad & truncate all sentences.\n",
        "                            pad_to_max_length = True,\n",
        "                            return_attention_mask = True,   # Construct attn. masks.\n",
        "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "        \n",
        "        input_ids.append(encoded_dict[\"input_ids\"])\n",
        "        attention_masks.append(encoded_dict[\"attention_mask\"])\n",
        "        if is_bert:\n",
        "            token_type_ids.append(encoded_dict[\"token_type_ids\"])\n",
        "\n",
        "        labels.append(c2l.str2int(item[\"label\"]))\n",
        "\n",
        "\n",
        "    # Convert the lists into tensors.\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    if is_bert:\n",
        "        token_type_ids = torch.cat(token_type_ids, dim=0)\n",
        "    labels = torch.tensor(labels)\n",
        "    \n",
        "\n",
        "\n",
        "    # Combine the training inputs into a TensorDataset.\n",
        "    if is_bert:\n",
        "        dataset = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n",
        "    else:\n",
        "        dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "w9kEajKpdZ6q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "qqJpziYsZ5e2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
        "\n",
        "def report_model_metrics(dataset, model_path, head_path=None):\n",
        "    tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base', do_lower_case=True)\n",
        "\n",
        "    if head_path:\n",
        "        model = torch.load(model_path)\n",
        "        head = torch.load(head_path)\n",
        "        model.eval()\n",
        "        head.eval()\n",
        "    else:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
        "\n",
        "    test_dataset = transform_to_embedding_dataset(tokenizer, dataset, is_bert=False)\n",
        "    test_sampler = SequentialSampler(test_dataset)\n",
        "    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=8)\n",
        "\n",
        "    dataset_logits = torch.Tensor().to(device)\n",
        "    truth_labels = torch.Tensor().to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1]}\n",
        "            if head_path:\n",
        "                outputs = model(**inputs)\n",
        "                last_hidden_state_cls = outputs[0][:, 0, :].to(device)\n",
        "                logits = torch.argmax(head(last_hidden_state_cls), dim=-1)\n",
        "            else:\n",
        "                logits = torch.argmax(model(**inputs).logits, dim=-1)\n",
        "            \n",
        "            dataset_logits = torch.cat((dataset_logits, logits), dim=0)\n",
        "            truth_labels = torch.cat((truth_labels, batch[2]), dim=0)\n",
        "\n",
        "    dataset_logits = dataset_logits.cpu().numpy()\n",
        "    truth_labels = truth_labels.cpu().numpy()\n",
        "\n",
        "    print(\"Metrics Report:\\n\")\n",
        "    print(f\"Accuracy: {accuracy_score(truth_labels, dataset_logits)}\")\n",
        "    print(f\"Confusion Matrix:\\n{confusion_matrix(truth_labels, dataset_logits)}\")\n",
        "    print(f\"Precision: {precision_score(truth_labels, dataset_logits)}\")\n",
        "    print(f\"Recall: {recall_score(truth_labels, dataset_logits)}\")\n",
        "    print(f\"F1-score: {f1_score(truth_labels, dataset_logits)}\")  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "mkPCrg6fw5Li",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "f49dad1c19ac41619ddd267e6821e384",
            "e01310c65115465ab59df3dab7077292",
            "a9fced86f6094e1995135568f09cc1c9",
            "d52c5e38bdb54773a0e2fc3f48bf1fa0",
            "e33f724e49954f4ca1bf03fd607315be",
            "7baa32bd017547d88fca9a7f3a5e0047",
            "f07308608ace4016aba54ee67368e96e",
            "c00313b35be6424b958b743ecb6b0364",
            "02fbb5eb00f840e98bbbb1ef596548d9",
            "09f374107c8041b294da784406995b68",
            "23c78f57e9d141629d093846346c4331"
          ]
        },
        "outputId": "d5b69aa9-3518-4178-c089-2b89202a0cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-a98fdd711d553a45/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f49dad1c19ac41619ddd267e6821e384"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "test_dataset = load_dataset(\"csv\", data_files=data_files)[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuPOvG190lNR"
      },
      "source": [
        "### Full Dataset Report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"./drive/MyDrive/Thesis/Data/experiments\""
      ],
      "metadata": {
        "id": "vfj6FhH60JRE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "412Z0tWH0hxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc6b8c7-1fc9-4373-9eb9-07ab1622087b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Evaluating: 100%|██████████| 188/188 [00:20<00:00,  9.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics Report:\n",
            "\n",
            "Accuracy: 0.924\n",
            "Confusion Matrix:\n",
            "[[1045   46]\n",
            " [  68  341]]\n",
            "Precision: 0.8811369509043928\n",
            "Recall: 0.8337408312958435\n",
            "F1-score: 0.8567839195979899\n"
          ]
        }
      ],
      "source": [
        "report_model_metrics(dataset=test_dataset, model_path=f\"{base_path}/full_dataset/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV_lhuGexoHE"
      },
      "source": [
        "### Random Sampling Report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report_model_metrics(dataset=test_dataset, model_path=f\"{base_path}/random_sampling/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwOb30Ax7M1T",
        "outputId": "3c4c7b6c-4123-468f-8fcf-13e9a2f6a33b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Evaluating: 100%|██████████| 188/188 [00:18<00:00, 10.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics Report:\n",
            "\n",
            "Accuracy: 0.91\n",
            "Confusion Matrix:\n",
            "[[997  94]\n",
            " [ 41 368]]\n",
            "Precision: 0.7965367965367965\n",
            "Recall: 0.8997555012224939\n",
            "F1-score: 0.8450057405281287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RysNkKNEyNef"
      },
      "source": [
        "### Breaking Ties Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "BVg70RKOyNCo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13166488-fd2e-4451-d882-dac22e2086e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Evaluating: 100%|██████████| 188/188 [00:18<00:00, 10.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics Report:\n",
            "\n",
            "Accuracy: 0.8926666666666667\n",
            "Confusion Matrix:\n",
            "[[985 106]\n",
            " [ 55 354]]\n",
            "Precision: 0.7695652173913043\n",
            "Recall: 0.8655256723716381\n",
            "F1-score: 0.8147295742232452\n"
          ]
        }
      ],
      "source": [
        "report_model_metrics(dataset=test_dataset, model_path=f\"{base_path}/breaking_ties/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMaKBugWyKTC"
      },
      "source": [
        "### Max Entropy Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4PAbPnzkyHFt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2781282a-e4d1-4b0a-99a8-83ae9bf21964"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Evaluating: 100%|██████████| 188/188 [00:18<00:00, 10.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics Report:\n",
            "\n",
            "Accuracy: 0.9066666666666666\n",
            "Confusion Matrix:\n",
            "[[1000   91]\n",
            " [  49  360]]\n",
            "Precision: 0.7982261640798226\n",
            "Recall: 0.8801955990220048\n",
            "F1-score: 0.8372093023255813\n"
          ]
        }
      ],
      "source": [
        "report_model_metrics(dataset=test_dataset, model_path=f\"{base_path}/max_entropy/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDJkzYb3xsMD"
      },
      "source": [
        "### Contrastive Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_EF7pW1FpJJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d280354f-32c3-4755-b8dd-68e752cbe2fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Evaluating: 100%|██████████| 188/188 [00:18<00:00,  9.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics Report:\n",
            "\n",
            "Accuracy: 0.9013333333333333\n",
            "Confusion Matrix:\n",
            "[[987 104]\n",
            " [ 44 365]]\n",
            "Precision: 0.7782515991471215\n",
            "Recall: 0.8924205378973105\n",
            "F1-score: 0.8314350797266514\n"
          ]
        }
      ],
      "source": [
        "report_model_metrics(dataset=test_dataset, model_path=f\"{base_path}/contrastive/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Least Confidence Report"
      ],
      "metadata": {
        "id": "COypHeqaGd-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report_model_metrics(dataset=test_dataset, model_path=f\"{base_path}/least_confidence/\")"
      ],
      "metadata": {
        "id": "RRTzZ8-UGarY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56a99374-bb18-4749-d03c-233acbaff0ff"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2346: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Evaluating: 100%|██████████| 188/188 [00:18<00:00,  9.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics Report:\n",
            "\n",
            "Accuracy: 0.9066666666666666\n",
            "Confusion Matrix:\n",
            "[[1000   91]\n",
            " [  49  360]]\n",
            "Precision: 0.7982261640798226\n",
            "Recall: 0.8801955990220048\n",
            "F1-score: 0.8372093023255813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discrepancy Report"
      ],
      "metadata": {
        "id": "cqTUHcSDQb6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report_model_metrics(dataset=test_dataset, \n",
        "                     model_path=f\"{base_path}/discrepancy/model.pt\", \n",
        "                     head_path=f\"{base_path}/discrepancy/head.pt\")"
      ],
      "metadata": {
        "id": "iA3lat7KpTHk",
        "outputId": "2e9ce3fb-92e8-4e71-a5bd-deba66092aba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Evaluating: 100%|██████████| 188/188 [00:18<00:00, 10.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics Report:\n",
            "\n",
            "Accuracy: 0.888\n",
            "Confusion Matrix:\n",
            "[[1046   45]\n",
            " [ 123  286]]\n",
            "Precision: 0.8640483383685801\n",
            "Recall: 0.6992665036674817\n",
            "F1-score: 0.772972972972973\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iFkNMv_14mmP",
        "QEpeXNGn4dUr",
        "z0ubhlGayHLj"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f49dad1c19ac41619ddd267e6821e384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e01310c65115465ab59df3dab7077292",
              "IPY_MODEL_a9fced86f6094e1995135568f09cc1c9",
              "IPY_MODEL_d52c5e38bdb54773a0e2fc3f48bf1fa0"
            ],
            "layout": "IPY_MODEL_e33f724e49954f4ca1bf03fd607315be"
          }
        },
        "e01310c65115465ab59df3dab7077292": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7baa32bd017547d88fca9a7f3a5e0047",
            "placeholder": "​",
            "style": "IPY_MODEL_f07308608ace4016aba54ee67368e96e",
            "value": "100%"
          }
        },
        "a9fced86f6094e1995135568f09cc1c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c00313b35be6424b958b743ecb6b0364",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_02fbb5eb00f840e98bbbb1ef596548d9",
            "value": 3
          }
        },
        "d52c5e38bdb54773a0e2fc3f48bf1fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09f374107c8041b294da784406995b68",
            "placeholder": "​",
            "style": "IPY_MODEL_23c78f57e9d141629d093846346c4331",
            "value": " 3/3 [00:00&lt;00:00, 66.90it/s]"
          }
        },
        "e33f724e49954f4ca1bf03fd607315be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7baa32bd017547d88fca9a7f3a5e0047": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f07308608ace4016aba54ee67368e96e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c00313b35be6424b958b743ecb6b0364": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02fbb5eb00f840e98bbbb1ef596548d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09f374107c8041b294da784406995b68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23c78f57e9d141629d093846346c4331": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}